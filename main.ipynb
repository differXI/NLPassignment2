{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 2\n",
    "## NLP Data Cleaning & Evaluation\n",
    "\n",
    "This notebook performs text cleaning, normalization, and evaluation using NLP techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import textstat\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"Sentiment Analysis Dataset.csv\"\n",
    "TEXT_COL = \"SentimentText\"\n",
    "\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "LEMMATIZER = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTICON_RE = re.compile(r\"(:\\)|:\\(|:D|;\\)|<3)\")\n",
    "SPECIAL_CHAR_RE = re.compile(r\"[^a-zA-Z\\s]\")\n",
    "PHONE_RE = re.compile(r\"\\b\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}\\b\")\n",
    "ACCOUNT_RE = re.compile(r\"\\b\\d{10,16}\\b\")\n",
    "ADDRESS_RE = re.compile(r\"\\b(street|road|rd|ave|avenue|lane|ln)\\b\", re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_stats(texts):\n",
    "    sentences, words = [], []\n",
    "    for t in texts:\n",
    "        sents = sent_tokenize(str(t))\n",
    "        sentences.extend(sents)\n",
    "        for s in sents:\n",
    "            words.extend(word_tokenize(s))\n",
    "    vocab = set(words)\n",
    "    sent_lengths = [len(word_tokenize(s)) for s in sentences if s.strip()]\n",
    "    return {\n",
    "        \"sentence_count\": len(sentences),\n",
    "        \"word_count\": len(words),\n",
    "        \"vocab_size\": len(vocab),\n",
    "        \"avg_sentence_length\": sum(sent_lengths) / len(sent_lengths),\n",
    "        \"max_sentence_length\": max(sent_lengths),\n",
    "        \"min_sentence_length\": min(sent_lengths),\n",
    "        \"max_word_length\": max(len(w) for w in words)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, counters):\n",
    "    text = str(text)\n",
    "    counters['emoticon_removed'] += len(EMOTICON_RE.findall(text))\n",
    "    counters['phone_removed'] += len(PHONE_RE.findall(text))\n",
    "    counters['account_removed'] += len(ACCOUNT_RE.findall(text))\n",
    "    counters['address_removed'] += len(ADDRESS_RE.findall(text))\n",
    "\n",
    "    text = EMOTICON_RE.sub(' ', text)\n",
    "    text = PHONE_RE.sub(' ', text)\n",
    "    text = ACCOUNT_RE.sub(' ', text)\n",
    "    text = ADDRESS_RE.sub(' ', text)\n",
    "\n",
    "    text = text.lower()\n",
    "    counters['special_char_removed'] += len(SPECIAL_CHAR_RE.findall(text))\n",
    "    text = SPECIAL_CHAR_RE.sub(' ', text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for t in tokens:\n",
    "        if t not in STOP_WORDS:\n",
    "            cleaned_tokens.append(LEMMATIZER.lemmatize(t))\n",
    "        else:\n",
    "            counters['stopword_removed'] += 1\n",
    "\n",
    "    counters['token_count'] += len(cleaned_tokens)\n",
    "    return ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readability_per_sentence(texts):\n",
    "    scores = []\n",
    "    for t in texts:\n",
    "        for s in sent_tokenize(str(t)):\n",
    "            if len(s.split()) > 2:\n",
    "                scores.append(textstat.flesch_kincaid_grade(s))\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(texts):\n",
    "    words = []\n",
    "    for t in texts:\n",
    "        words.extend(word_tokenize(str(t)))\n",
    "    return len(set(words)) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH, encoding='latin1')\n",
    "original_texts = df[TEXT_COL].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'sentence_count': 1814099,\n",
       "  'word_count': 17582802,\n",
       "  'vocab_size': 696649,\n",
       "  'avg_sentence_length': 9.692305656968005,\n",
       "  'max_sentence_length': 229,\n",
       "  'min_sentence_length': 1,\n",
       "  'max_word_length': 136},\n",
       " 4.082191271973262,\n",
       " 0.039630998459033694)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before_stats = basic_stats(original_texts)\n",
    "before_readability = readability_per_sentence(original_texts)\n",
    "before_lexical = lexical_diversity(original_texts)\n",
    "before_stats, before_readability, before_lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "counters = {\n",
    "    'emoticon_removed': 0,\n",
    "    'stopword_removed': 0,\n",
    "    'token_count': 0,\n",
    "    'special_char_removed': 0,\n",
    "    'phone_removed': 0,\n",
    "    'account_removed': 0,\n",
    "    'address_removed': 0\n",
    "}\n",
    "\n",
    "df['clean_text'] = df[TEXT_COL].apply(lambda x: clean_text(x, counters))\n",
    "clean_texts = df['clean_text'].dropna().tolist()\n",
    "runtime = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'sentence_count': 1048384,\n",
       "  'word_count': 8192895,\n",
       "  'vocab_size': 476685,\n",
       "  'avg_sentence_length': 7.81478446828643,\n",
       "  'max_sentence_length': 78,\n",
       "  'min_sentence_length': 1,\n",
       "  'max_word_length': 117},\n",
       " 6.425984062831788,\n",
       " 0.058182730280322156)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_stats = basic_stats(clean_texts)\n",
    "after_readability = readability_per_sentence(clean_texts)\n",
    "after_lexical = lexical_diversity(clean_texts)\n",
    "after_stats, after_readability, after_lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for s in clean_texts:\n",
    "        f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning: {'sentence_count': 1814099, 'word_count': 17582802, 'vocab_size': 696649, 'avg_sentence_length': 9.692305656968005, 'max_sentence_length': 229, 'min_sentence_length': 1, 'max_word_length': 136}\n",
      "After Cleaning: {'sentence_count': 1048384, 'word_count': 8192895, 'vocab_size': 476685, 'avg_sentence_length': 7.81478446828643, 'max_sentence_length': 78, 'min_sentence_length': 1, 'max_word_length': 117}\n",
      "Readability Before: 4.08\n",
      "Readability After : 6.43\n",
      "Lexical Diversity Before: 0.040\n",
      "Lexical Diversity After : 0.058\n",
      "Runtime: 157.66 seconds\n",
      "Counters: {'emoticon_removed': 5860, 'stopword_removed': 6424510, 'token_count': 8192895, 'special_char_removed': 5051045, 'phone_removed': 356, 'account_removed': 361, 'address_removed': 2596}\n"
     ]
    }
   ],
   "source": [
    "print('Before Cleaning:', before_stats)\n",
    "print('After Cleaning:', after_stats)\n",
    "print(f'Readability Before: {before_readability:.2f}')\n",
    "print(f'Readability After : {after_readability:.2f}')\n",
    "print(f'Lexical Diversity Before: {before_lexical:.3f}')\n",
    "print(f'Lexical Diversity After : {after_lexical:.3f}')\n",
    "print(f'Runtime: {runtime:.2f} seconds')\n",
    "print('Counters:', counters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
